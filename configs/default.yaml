# Activation-sequence probe pipeline — default config (dev: distilgpt2, RS1)
seed: 42

# Data: CSV layout under data_dir, labels from folder
data:
  data_dir: data/RS1
  categories: [benign, malicious]
  label_map:
    benign: 0
    malicious: 1
  split_by_file: false
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  limit_docs: null  # set to int for quick runs

# Base LM (whose activations we extract). Switch by changing base_model_name (e.g. gpt2, distilgpt2, meta-llama/Llama-3.1-8B-Instruct).
# cache_dir: use a path inside the repo (e.g. .cache/huggingface) if ~/.cache has permission issues.
model:
  base_model_name: distilgpt2
  cache_dir: .cache/huggingface
  layer_index: 3
  tap_point: resid_post  # resid_post | resid_pre | mlp_out

# Extraction
extraction:
  max_seq_len: 256
  stride: 128
  batch_size_extract: 8
  activation_dtype: float32
  memmap_dir: outputs/activations
  limit_docs: null

# Layer search: which layers to try, metric to pick best
layer_search:
  layer_search_layers: [0, 1, 2, 3, 4, 5]
  layer_search_metric: macro_f1
  layer_search_output: outputs/best_layer.json

# ActFormer
actformer:
  d_model: 256
  n_layers: 2
  n_heads: 4
  dropout: 0.1
  ff_mult: 4
  loss_type: gaussian_nll  # mse | gaussian_nll
  normalization: standardize_per_dim  # layernorm | standardize_per_dim
  causal: false
  min_subseq_len: 2
  max_subseq_len: 256
  augment:
    noise_std: 0.0
    input_dropout: 0.0

pretrain:
  lr: 1e-4
  wd: 0.01
  epochs: 3
  batch_size: 32
  grad_clip: 1.0
  warmup_steps: 100
  eval_every: 200
  save_every: 500
  max_steps: null  # override to limit steps (e.g. 50 for tests)
  output_dir: outputs/actformer

# Probe (classifier trained on pooled activations). linear | mlp | transformer (transformer = from scratch).
probe:
  pooling: mean  # last | mean | attention_pool
  probe_model: linear  # linear | mlp | transformer
  transformer_probe:  # used when probe_model: transformer
    d_model: 256
    n_layers: 2
    n_heads: 4
    ff_mult: 4
    dropout: 0.1
    pool: mean
  probe_train:
    lr: 1e-3
    epochs: 50
    batch_size: 32
    l2: 0.01
  augment:
    noise_std: 0.0
    mixup_alpha: 0.0
  baselines_on_raw_activations: true

# Probe comparison (ID vs OOD) — paths filled after extraction
comparison:
  activations:
    id:
      memmap_dir: outputs/activations/id
      index_path: outputs/activations/id/index.json
    ood: []
  actformer_checkpoint: outputs/actformer/best.pt
  probe_types: [raw_linear, raw_mlp, raw_transformer, actformer_linear, actformer_mlp, actformer_transformer]
  pooling: mean
  probe_train:
    lr: 1e-3
    epochs: 50
    batch_size: 32
    l2: 0.01
  metrics: [accuracy, macro_f1, auroc]
  output_dir: outputs/probe_comparison
